{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "vC_MPPIPBBk_",
      "metadata": {
        "id": "vC_MPPIPBBk_"
      },
      "source": [
        "# Projet de collecte de données sur des sites web : Recherche de CVEs\n",
        "*ALLAGLO Giovanni*\\\n",
        "*Etudiant Ingénieur Informatique et Cybersécurité*\n",
        "\n",
        "Année académique : 2023-2024\n",
        "\n",
        "lien vers le notebook : [⇲---↱](https://colab.research.google.com/drive/1KWdobNcrMR2K17kMVmd9GyArtbH2CxLs?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mu7NUFT1JJBh",
      "metadata": {
        "id": "Mu7NUFT1JJBh"
      },
      "source": [
        "## Contexte\n",
        "\n",
        "La majorité des entreprises de nos jours sont confrontées au défi majeur de maintenir la résilience de leur système d'information face aux cybermenaces. Dans ce cadre, je me propose de mettre au point un mini-projet dont le but serait de collecter dans une base de données les vulnérabilités critiques (récemment exploitées ou découvertes), les exploits et les actualités du secteur.\n",
        "\n",
        "## Objectif\n",
        "\n",
        "L'objectif ultime de ce projet est d'établir un service interopérable avec un système d'information, capable de déclencher des alertes en lien avec les technologies du SI touchées par les actualités relatives aux tendances des cyber-menaces et aux vulnérabilités critiques.\n",
        "\n",
        "\\\n",
        "## Déroulement du projet (Milestones)\n",
        "\n",
        "### Partie 1: Collecte de données\n",
        "- Etape 1 : Mise en  place d'un script de Web Scraping\n",
        "- Etape 2 : Application de tests d'evasion ( afin de verifier la capacité de notre robots à éviter la detection )\n",
        "\n",
        "### Parti 2: Persistence des données et fonctionnalités de recherche\n",
        "- Etape1 : Création d'une base de données avec SQLAlquemy\n",
        "- Etape2 : Mise en place d'un moteur de recherche"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vAa76Se8KqCF",
      "metadata": {
        "id": "vAa76Se8KqCF"
      },
      "source": [
        "##Requirements\n",
        "\n",
        "Les librairies et bibliothèques utiles seront mentionnées dans le fichier `requirements.txt`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b74a2def",
      "metadata": {},
      "source": [
        "- import requests\n",
        "- import re\n",
        "- import bs4\n",
        "- from bs4 import BeautifulSoup\n",
        "- import pandas as pd\n",
        "- import yaml\n",
        "- from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UEQeyi6zMGY3",
      "metadata": {
        "id": "UEQeyi6zMGY3"
      },
      "source": [
        "## **Partie 1: Collecte de données**\n",
        "\n",
        "Notre travail se base sur 3 sources :\n",
        " - [le site du CERT du gouvernement français](https://www.cert.ssi.gouv.fr/a-propos)\n",
        " - [Le site maintenu par le NIST et contenant la base de données sur les vulnérabilités repertoriées](https://nvd.nist.gov/general)\n",
        " - [Le site maintenu par Offsec et contenant des archives d'exploits-CVE](https://www.exploit-db.com/about-exploit-db)\n",
        "\n",
        "\\\n",
        "Le choix de ces sources s'est fait en se basant sur leurs divers apports en données.\n",
        "La première source, CERT-FR, fournit différents éléments concernant l'actualité (alertes de sécurité, menaces) ainsi que des recommandations en matière de sécurité.\n",
        "\n",
        "La deuxième source, NVD, est un site alimenté par le National Institute of Standards and Technology et constitue un dictionnaire de CVEs. La page web sur laquelle nous travaillerons nous permettra de recueillir les 20 dernières vulnérabilités enregistrées ainsi que leur score de gravité (CVSS).\n",
        "\n",
        "La troisième et dernière source (*facultative*) recueille les exploits de vulnérabilités et constitue un Proof of Concept(POC) sur les exploits. Ces données sont souvent utiles aux équipes d'audits.\n",
        "\n",
        "\\\n",
        "\n",
        "Les informations relatives à ces sources (URL, description du site) seront enregistrées dans le fichier `source_URLs.yml`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2jJ-hYfUv44o",
      "metadata": {
        "id": "2jJ-hYfUv44o"
      },
      "source": [
        "### **Mise en place des scripts pour la collecte des données : WebScrapping**\n",
        "\n",
        "Dans cette partie du projet, voici les principales bibliothèques :\n",
        "- `requests` : *Pour effectuer les requêtes HTTP*\n",
        "- `pandas`   : *Pour la manipulation des données*\n",
        "- `re`       : *Pour les expressions régulières*\n",
        "- `bs4`      : *Pour analyser les balises HTML des sites web*\n",
        "- `yaml`     : *Pour la manipulation des fichiers .yml*\n",
        "\n",
        "\\\n",
        "Dans une optique fonctionnelle, nous avons créé un module pour chacune des tâches de scraping : `nvd_scrapping.py`, `certfr_scrapping.py`. Chaque module contient des fonctions (*de lecture et de normalisation*) ainsi qu'une classe qui hérite de `ScrapperInterface`.\n",
        "\n",
        "\\\n",
        "La classe mère `ScrapperInterface` peut être vue comme une interface qui définit la méthode de classe `parser()` retournant les données collectées à l'issue du scraping. Elle possède également deux attributs de classe, `URL` et `SOURCE` (*initialisés à None*), qui représentent respectivement l'URL et le nom du site web ciblé. Ainsi, toutes les classes filles héritent de ces attributs et implémentent le comportement de `ScrapperInterface`.\n",
        "\n",
        "\\\n",
        "Le module `nvd_scrapping.py` contient la classe `NvdScrapper` qui implémente `ScrapperInterface`. Ce module permet de faire du scraping sur le site de la National Vulnerability Database.\n",
        "\n",
        "Le module `certfr_scrapping.py` contient la classe `CertfrScrapper` qui implémente également `ScrapperInterface`. Ce module permet de faire du scraping sur le site de CERT-FR."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gtKeMkTWLkD3",
      "metadata": {
        "id": "gtKeMkTWLkD3"
      },
      "source": [
        "#### **Démarche & Fonctionnement de la méthode `parser()`**\n",
        "\n",
        "1. Localisation des balises HTML grâce à l'inspecteur de page intégré dans le navigateur (Firefox, Chrome, etc.).\n",
        "\n",
        "2. Requête GET sur l'URL et récupération de la page HTML.\n",
        "\n",
        "3. Utilisation de `BeautifulSoup` pour parser le HTML.\n",
        "\n",
        "4. Mise en place de fonctions de lecture et de normalisation des résultats (`normalise_url`: transforme un chemin relatif d'URL en son équivalent chemin absolu, `normalise_date`: met la date au format standard `YYYY-MM-DD HH:MM:SS`, etc.).\n",
        "\n",
        "NB : La première étape vise à repérer l'emplacement précis des données dans la page HTML, une démarche essentielle pour définir avec précision la fonction `parser()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0_h0vVEKAeWQ",
      "metadata": {
        "id": "0_h0vVEKAeWQ"
      },
      "source": [
        "En fonction des cas, la fonction `parser` prend facultativement en paramètre un *header* pour l'envoi de la requête et retourne une DataFrame (ou une liste de DataFrames) pandas avec les informations extraites du site cible.\n",
        "\n",
        "> Pour la classe `NvdScrapper`, elle retourne des informations sur les 20 dernières CVE répertoriées par l'Institut (NIST), telles que l'identifiant, une description brève, un lien vers une ressource pour obtenir plus de détails, la date de publication, la version CVSS utilisée pour le scoring, le score CVSS, et la source (*cette information se trouve initialement dans le fichier .yml*).\n",
        "\n",
        "> Pour la classe `CertfrScrapper`, elle retourne une liste de DataFrames, chacun contenant les informations sur une rubrique (les alertes, les recommandations, les actualités et les menaces). Chaque DataFrame contient les informations de l'identifiant de l'élément de la rubrique, de sa date de publication, du titre et de son statut (dans le cas des alertes de sécurité).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zm-U-VdzAtYC",
      "metadata": {
        "id": "Zm-U-VdzAtYC"
      },
      "source": [
        "#### Tests et Visualisation des données\n",
        "\n",
        "Deux façons d'obtenir les résultats :\n",
        "\n",
        "1. Utiliser la méthode `parser()`.\n",
        "2. Créer une instance de la classe.\n",
        "\n",
        "NB : En instanciant une classe, nous récupérons les données ainsi que la date de collecte via les attributs `data` et `data_collect_time`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "H9yfcGM9gg8r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9yfcGM9gg8r",
        "outputId": "9b958ad4-99f8-4332-82b1-f42abf0dadf0"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'scrapping_interface'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#import des classes\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnvd_scrapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NvdScrapper\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcertfr_scrapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CertfrScrapper\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#lecture des sources dans le fichier YAML\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Documents\\Python Scripts\\WebScraper\\scripts\\nvd_scrapping.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscrapping_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ScrapperInterface\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNvdScrapper\u001b[39;00m(ScrapperInterface):\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    A class to scrape vulnerability data from the NVD (National Vulnerability Database).\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m        Exctract vulnerability data from the NVD website and returns a Pandas DataFrame.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scrapping_interface'"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "#import des classes\n",
        "from scripts.nvd_scrapping import NvdScrapper\n",
        "from scripts.certfr_scrapping import CertfrScrapper\n",
        "\n",
        "#lecture des sources dans le fichier YAML\n",
        "with open(\"files/source_URLs.yml\") as sources_file:\n",
        "    sources = yaml.safe_load(sources_file)\n",
        "\n",
        "#contenu du fichier yaml\n",
        "print(sources)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rpHGvRBoL7Z6",
      "metadata": {
        "id": "rpHGvRBoL7Z6"
      },
      "source": [
        "##### test sur le site **nvd.nist.gov**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "t7AXaKaJ6ECe",
      "metadata": {
        "id": "t7AXaKaJ6ECe"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'sources' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# data from NVD|NIST\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43msources\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNvdScrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m source \u001b[38;5;241m=\u001b[39m sources[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNvdScrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Mise à jour des valeurs url et source\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'sources' is not defined"
          ]
        }
      ],
      "source": [
        "# data from NVD|NIST\n",
        "url = sources[\"NvdScrapper\"][\"url\"]\n",
        "source = sources[\"NvdScrapper\"][\"source\"]\n",
        "\n",
        "#Mise à jour des valeurs url et source\n",
        "NvdScrapper.URL = url\n",
        "NvdScrapper.SOURCE = source\n",
        "\n",
        "# Test\n",
        "\n",
        "#Test : Instance de classe\n",
        "instance_nvd = NvdScrapper()\n",
        "data_nvd1 = instance_nvd.data\n",
        "print(f\"date de collecte {instance_nvd.data_collect_time} \")\n",
        "\n",
        "#Test : Usage de la méthode de classe\n",
        "data_nvd2 = NvdScrapper.parser()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "rzJRgerVMz6T",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "rzJRgerVMz6T",
        "outputId": "e06a3eda-310d-41c3-e981-0be698fe2380"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data_nvd2' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata_nvd2\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'data_nvd2' is not defined"
          ]
        }
      ],
      "source": [
        "data_nvd2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VxVyccgnNmhk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "VxVyccgnNmhk",
        "outputId": "c2c976c5-a656-41af-8e20-7753e96f2e1e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Xaas' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n Xaas ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "data_nvd1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5yq1jPpxML3F",
      "metadata": {
        "id": "5yq1jPpxML3F"
      },
      "source": [
        "##### test sur le site **cert.ssi.gouv.fr**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "A3vYEywuh3dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3vYEywuh3dd",
        "outputId": "f28f07b9-d05e-4254-a74d-c0695192e591"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'sources' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#data from CERT-FR\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43msources\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCertfrScrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m source \u001b[38;5;241m=\u001b[39m sources[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCertfrScrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m CertfrScrapper\u001b[38;5;241m.\u001b[39mURL \u001b[38;5;241m=\u001b[39m url\n",
            "\u001b[1;31mNameError\u001b[0m: name 'sources' is not defined"
          ]
        }
      ],
      "source": [
        "#data from CERT-FR\n",
        "url = sources[\"CertfrScrapper\"][\"url\"]\n",
        "source = sources[\"CertfrScrapper\"][\"source\"]\n",
        "CertfrScrapper.URL = url\n",
        "CertfrScrapper.SOURCE = source\n",
        "\n",
        "#Test\n",
        "\n",
        "instance_cert = CertfrScrapper()\n",
        "data_cert1 = instance_cert.data\n",
        "print(f\"date de collecte {instance_nvd.data_collect_time} \")\n",
        "\n",
        "data_cert2 = CertfrScrapper.parser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MMH-lLbkN0LW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "MMH-lLbkN0LW",
        "outputId": "4d888054-4b19-4b0d-ed48-a8d7e4737a70"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Xaas' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n Xaas ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "data_cert1[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89mdg7cENroS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "89mdg7cENroS",
        "outputId": "e6c7d22c-a9b2-47c2-cf98-bffa90d6ed7a"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Xaas' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n Xaas ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "data_cert1[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4slYDapN6XI0",
      "metadata": {
        "id": "4slYDapN6XI0"
      },
      "source": [
        "### Tests d'evasion : Autres approches & Utilisation de selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8S9qlYilPi2g",
      "metadata": {
        "id": "8S9qlYilPi2g"
      },
      "source": [
        "Nous n'avons pas eu besoin d'utiliser des techniques d'évasion dans cette première partie du projet. Cependant, gardons à l'esprit que cela devient indispensable lorsque nous cherchons à effectuer des requêtes massives (centaines de requêtes et plus) sur une URL. Certains sites utilisent des techniques telles que les CAPTCHA et reCAPTCHA lorsqu'ils suspectent des connexions provenant d'un robot (connexion excessive depuis une même IP ou un même proxy).\n",
        "\n",
        "\\\n",
        "Plusieurs techniques existent pour simuler une connexion normale :\n",
        "\n",
        "- Modifier les données d'en-tête des requêtes (modification du user-agent, par exemple).\n",
        "- Utilisation de différents proxies.\n",
        "- Utilisation de la bibliothèque `selenium` pour simuler une requête émanant d'un navigateur. Cette dernière peut être combinée avec les deux méthodes précédentes.\n",
        "- Utilisation du Framework Scrapy (utile par sa capacité à contourner les CAPTCHAs et d'autres mesures anti-scraping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uXqVOPzZS9pb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXqVOPzZS9pb",
        "outputId": "47db5f45-dfc3-498e-b16d-bd0629f26f9b"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Xaas' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n Xaas ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import random\n",
        "# Test avec des user-agents pris aléatoirement depuis notre fichier YAML\n",
        "with open(\"files/headers.yml\") as f_headers:\n",
        "    content = yaml.safe_load(f_headers)\n",
        "\n",
        "#Constructions d'un header\n",
        "browser = random.choice([\"Chrome\",\"Firefox\"])\n",
        "user_agents = content[\"user-agent\"][browser]\n",
        "\n",
        "header = content[\"header\"][browser]\n",
        "header[\"User-Agent\"] = random.choice(user_agents)\n",
        "print(\"en-tête utilisée: \",header)\n",
        "\n",
        "#Test : Instance de classe\n",
        "instance_nvd = NvdScrapper(header)\n",
        "data_nvd1 = instance_nvd.data\n",
        "print(f\"date de collecte : {instance_nvd.data_collect_time} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uSUyUE4_ONlq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "uSUyUE4_ONlq",
        "outputId": "8ef0eeb7-9330-4c06-99c0-11f405666988"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Xaas' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n Xaas ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "data_nvd1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SCliXb5P6j9U",
      "metadata": {
        "id": "SCliXb5P6j9U"
      },
      "source": [
        "### Conclusion et Retour d'expérience sur la Partie 1\n",
        "\n",
        "- Risque de blocage d'accès au site web par le programme Python lors de l'envoi répété de requêtes avec les mêmes données d'en-tête.\n",
        "\n",
        "  > Solution : Ajuster les en-têtes | Utiliser le Framework Selenium | Utilisation du Framework Scrapy\n",
        "\n",
        "- La langue dans laquelle les données sont renvoyées peut changer en fonction du header, ce qui peut entraîner des erreurs lorsqu'on attend un pattern regex spécifique.\n",
        "\n",
        "  > Solution : Spécifier la préférence linguistique dans le header.\n",
        "\n",
        "- Des erreurs peuvent survenir en fonction de l'état ou des modifications sur le site web cible.\n",
        "\n",
        "  > Solution : Implémenter une gestion des erreurs (bloc try-exception, utilisation de conditionnelles).\n",
        "\n",
        "Par habitude et pour assurer la cohérence logique du code, nous optons pour le typage des variables et des fonctions. Des tests fonctionnels sont également mis en place."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Diaporama",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
